[Unit]
Description=llama.cpp server (Nemotron-3-Nano)
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
WorkingDirectory=%h/llama.cpp

# Pin to A720 cores only
CPUAffinity=0 1 6 7 8 9 10 11

# Keep generation on 8 threads (your bench says this is best)
ExecStart=%h/llama.cpp/build/bin/llama-server \
  --models-dir %h/models \
  --models-max 1 \
  --ctx-size 16384 \
  --cache-type-k q8_0 \
  --cache-type-v q8_0 \
  --threads 8 \
  --threads-batch 8 \
  --batch-size 1024 \
  --ubatch-size 256 \
  --parallel 1 \
  --host 0.0.0.0 \
  --port 8765 \
  --sleep-idle-seconds 600

Restart=on-failure
RestartSec=2
Nice=-5

[Install]
WantedBy=default.target

